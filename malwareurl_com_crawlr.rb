#!/usr/bin/env ruby
require 'crawlr'

Crawlr::Processor.new(ARGV[0]).start do |crawlr|
  # site requires registration and they send you a unique url.
  # working with the export of domains, urls and additional details
  crawlr.crawl_page('http://www.malwareurl.com/reg-export-urls.php?export=urls2&key=gwhz9if8n4f9z8x9xnudk8t4y7') do |page|
    page.body.each_line do |line|
      href, ip, reverse_name, asn, av_info, date = *line.split(/,/)
      url = "http://#{href}"
      puts "Fetching #{url}"
      crawlr.site_agent.get_page(url) do |malware_page|
        if malware_page.is_ok? && !malware_page.content_type_class?('text/')
          crawlr.store(malware_page, true, av_info)
        end
      end unless crawlr.stored? url
    end
  end
end