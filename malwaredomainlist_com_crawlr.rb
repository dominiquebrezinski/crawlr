#!/usr/bin/env ruby
require 'crawlr'

Crawlr::Processor.new.start do |crawlr|
  crawlr.create_agent_for_site('http://www.malwaredomainlist.com/')
  # the listings are paginated, so we grab the first page, pull out the highest index,
  # and add all the pages to the run queue.
  first_page = crawlr.site_agent.get_page('http://www.malwaredomainlist.com/mdl.php?inactive=&sort=Date&search=&colsearch=All&ascordesc=DESC&quantity=100&page=0')
  last_page_index = first_page.search('body/div[@class="MainContainer"]/div[@class="ContentBox"]/center/p/a[@href][last()]/text()').first.to_s.to_i
  puts "last index found: #{last_page_index}"
  (0..last_page_index).each do |index|
    crawlr.site_agent.get_page("http://www.malwaredomainlist.com/mdl.php?inactive=&sort=Date&search=&colsearch=All&ascordesc=DESC&quantity=100&page=#{index}") do |page|
      # listings are in a table, so pull each row that is not the header row
      page.search('body/div[@class="MainContainer"]/div[@class="ContentBox"]/table//tr[@bgcolor]').each do |entry|
        # they inserted <wbr> tags in the url and av info fields to confound automated processing :/
        # so we pull the first and last text elements for each field
        href = entry.search("td[2]//text()").map{|t| t.to_s }.join
        # sometimes the url is in the third field :/
        if href.empty? || href =~ /^-/
          href = entry.search("td[3]//text()").map{|t| t.to_s }.join
        end
        av_info = entry.search("td[5]//text()").map{|t| t.to_s }.join
        url = "http://#{href}"
        unless href =~ /^-/ || crawlr.stored?(url)
          puts "Fetching #{url}"
          crawlr.site_agent.get_page(url) do |malware_page|
            if malware_page.is_ok? && !malware_page.content_type_class?('text/')
              crawlr.store(malware_page, true, av_info)
            end
          end
        end
      end
    end
  end
end